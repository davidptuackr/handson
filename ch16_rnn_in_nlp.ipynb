{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e71355d-b15f-427d-97db-fa4dfd04670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ba9315-c273-4cf4-8eaf-5ac538188162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://homl.info/shakespeare\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f598488-cc87-4556-b16a-61da713a21b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3061b8a2-c665-4080-9a05-9e871b881dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    split=\"character\",\n",
    "    standardize=\"lower\"\n",
    ")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42d9191-17fc-40ef-9618-2370c66ffe81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 1115394)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "n_tokens, dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6e67e6-5eb6-4b3f-9ac8-986394077fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length+1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length+1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100000, seed=seed)\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f03ca71-23bf-46d3-8c21-17a69e23614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = to_dataset(\n",
    "    encoded[:1_000_000], \n",
    "    length=length,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "valid_set = to_dataset(\n",
    "    encoded[1_000_000 : 1_060_000], length=length\n",
    ")\n",
    "test_set = to_dataset(\n",
    "    encoded[1_060_000 : ], length=length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad5d66d9-595f-49e8-8d54-7eedcec82381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  31247/Unknown \u001b[1m938s\u001b[0m 30ms/step - accuracy: 0.5460 - loss: 1.5038"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davidbowman\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31247/31247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 30ms/step - accuracy: 0.5460 - loss: 1.5038 - val_accuracy: 0.5338 - val_loss: 1.6068\n",
      "Epoch 2/10\n",
      "\u001b[1m31247/31247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m965s\u001b[0m 31ms/step - accuracy: 0.5978 - loss: 1.2919 - val_accuracy: 0.5405 - val_loss: 1.5807\n",
      "Epoch 3/10\n",
      "\u001b[1m 9196/31247\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:30\u001b[0m 31ms/step - accuracy: 0.6008 - loss: 1.2740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AtomicFunction.__del__ at 0x000001D44FA25620>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\davidbowman\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 286, in __del__\n",
      "    def __del__(self):\n",
      "  \n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model.keras\", # \"모델 전체 저장 -> .keras로 끝날 것\"\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    epochs=10,\n",
    "    callbacks=[model_ckpt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13246a89-f336-4b6f-9751-ddd301ba3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X-2),\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38cae52-c84e-423c-821d-847a109821f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)\n",
    "text_vec_layer.get_vocabulary()[y_pred+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd206849-71fb-458c-9caf-f9cc0108bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
